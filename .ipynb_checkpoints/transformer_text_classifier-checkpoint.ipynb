{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8cfdedc",
   "metadata": {},
   "source": [
    "! pip install tensorflow-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d053de4",
   "metadata": {},
   "source": [
    "! pip install keras_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7bf6dd",
   "metadata": {},
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d114e6b",
   "metadata": {},
   "source": [
    "import keras_nlp\n",
    "print(keras_nlp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d9698",
   "metadata": {},
   "source": [
    "import tensorflow_text\n",
    "print(tensorflow_text.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6471bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea10986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing params.\n",
    "PRETRAINING_BATCH_SIZE = 128\n",
    "FINETUNING_BATCH_SIZE = 32\n",
    "\n",
    "SEQ_LENGTH = 512\n",
    "MASK_RATE = 0.25\n",
    "PREDICTIONS_PER_SEQ = 32\n",
    "\n",
    "# Model params.\n",
    "NUM_LAYERS = 3\n",
    "MODEL_DIM = 256\n",
    "INTERMEDIATE_DIM = 512\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.1\n",
    "NORM_EPSILON = 1e-5\n",
    "\n",
    "# Training params.\n",
    "PRETRAINING_LEARNING_RATE = 5e-4\n",
    "PRETRAINING_EPOCHS = 8\n",
    "FINETUNING_LEARNING_RATE = 5e-5\n",
    "FINETUNING_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe9276",
   "metadata": {},
   "source": [
    "# Download vocabulary data.\n",
    "vocab_file = keras.utils.get_file(\n",
    "    origin=\"https://storage.googleapis.com/tensorflow/keras-nlp/examples/bert/bert_vocab_uncased.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efd5b09",
   "metadata": {},
   "source": [
    "# Setting sequence_length will trim or pad the token outputs to shape\n",
    "# (batch_size, SEQ_LENGTH).\n",
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab_file,\n",
    "    sequence_length=SEQ_LENGTH,\n",
    "    lowercase=True,\n",
    "    strip_accents=True,\n",
    ")\n",
    "\n",
    "# Setting mask_selection_length will trim or pad the mask outputs to shape\n",
    "# (batch_size, PREDICTIONS_PER_SEQ).\n",
    "masker = keras_nlp.layers.MLMMaskGenerator(\n",
    "    vocabulary_size=tokenizer.vocabulary_size(),\n",
    "    mask_selection_rate=MASK_RATE,\n",
    "    mask_selection_length=PREDICTIONS_PER_SEQ,\n",
    "    mask_token_id=tokenizer.token_to_id(\"[MASK]\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e478ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04c99dd",
   "metadata": {},
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91fada9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(\n",
    "    seq_length = SEQ_LENGTH,\n",
    "    ):\n",
    "    inputs = keras.Input(shape=(seq_length,), dtype=tf.int32)\n",
    "\n",
    "    # Embed our tokens with a positional embedding.\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "        vocabulary_size=tokenizer.vocab_size,\n",
    "        sequence_length=seq_length,\n",
    "        embedding_dim=MODEL_DIM,\n",
    "    )\n",
    "    outputs = embedding_layer(inputs)\n",
    "\n",
    "    # Apply layer normalization and dropout to the embedding.\n",
    "    outputs = keras.layers.LayerNormalization(epsilon=NORM_EPSILON)(outputs)\n",
    "    outputs = keras.layers.Dropout(rate=DROPOUT)(outputs)\n",
    "\n",
    "    # Add a number of encoder blocks\n",
    "    outputs = keras_nlp.layers.TransformerEncoder(\n",
    "        intermediate_dim=INTERMEDIATE_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        layer_norm_epsilon=NORM_EPSILON,\n",
    "    )(outputs)\n",
    "\n",
    "    encoder_model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    return encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd0bd8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_text_single_label_classifier(\n",
    "    num_label = 2,\n",
    "    seq_length = SEQ_LENGTH,\n",
    "    ):\n",
    "    \n",
    "    encoder_model = transformer_encoder(\n",
    "        seq_length,\n",
    "        )\n",
    "\n",
    "    # Take as input the tokenized input.\n",
    "    inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32)\n",
    "\n",
    "    # Encode and pool the tokens.\n",
    "    encoded_tokens = encoder_model(inputs)\n",
    "    pooled_tokens = keras.layers.GlobalAveragePooling1D()(encoded_tokens)\n",
    "\n",
    "    # Predict an output label.\n",
    "    outputs = keras.layers.Dense(num_label, activation=\"softmax\")(pooled_tokens)\\\n",
    "    \n",
    "    finetuning_model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    return finetuning_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "643f8ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_text_multi_label_classifier(\n",
    "    num_label = 2,\n",
    "    seq_length = SEQ_LENGTH,\n",
    "    ):\n",
    "    \n",
    "    encoder_model = transformer_encoder(\n",
    "        seq_length,\n",
    "        )\n",
    "\n",
    "    # Take as input the tokenized input.\n",
    "    inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32)\n",
    "\n",
    "    # Encode and pool the tokens.\n",
    "    encoded_tokens = encoder_model(inputs)\n",
    "    pooled_tokens = keras.layers.GlobalAveragePooling1D()(encoded_tokens)\n",
    "\n",
    "    # Predict an output label.\n",
    "    outputs = keras.layers.Dense(num_label, activation=\"sigmoid\")(pooled_tokens)\\\n",
    "    \n",
    "    finetuning_model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    return finetuning_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
